name: Benchmarks

on:
  push:
    branches: [main, master]
    paths:
      - "src/**"
      - "benchmarks/**"
      - "pyproject.toml"
  pull_request:
    branches: [main, master]
    paths:
      - "src/**"
      - "benchmarks/**"
      - "pyproject.toml"
  workflow_dispatch:
    inputs:
      compare_baseline:
        description: "Compare against baseline"
        required: false
        default: "true"
        type: boolean

permissions:
  contents: write
  pull-requests: write

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-benchmark-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-benchmark-
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Download baseline (if exists)
        uses: actions/cache@v4
        id: benchmark-cache
        with:
          path: .benchmark_baseline.json
          key: benchmark-baseline-${{ github.ref_name }}
          restore-keys: |
            benchmark-baseline-main
            benchmark-baseline-

      - name: Run benchmarks
        run: |
          pytest benchmarks/ \
            --benchmark-only \
            --benchmark-json=benchmark_results.json \
            --benchmark-columns=min,max,mean,stddev,rounds \
            --benchmark-sort=mean \
            --benchmark-warmup=on \
            --benchmark-disable-gc \
            -v

      - name: Compare with baseline
        if: steps.benchmark-cache.outputs.cache-hit == 'true'
        run: |
          echo "## Benchmark Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python -c "
          import json
          import sys

          try:
              with open('.benchmark_baseline.json') as f:
                  baseline = json.load(f)
              with open('benchmark_results.json') as f:
                  current = json.load(f)

              baseline_map = {b['name']: b for b in baseline.get('benchmarks', [])}
              current_map = {b['name']: b for b in current.get('benchmarks', [])}

              print('| Benchmark | Baseline (ms) | Current (ms) | Change |')
              print('|-----------|---------------|--------------|--------|')

              regressions = []
              for name, curr in current_map.items():
                  base = baseline_map.get(name)
                  curr_mean = curr['stats']['mean'] * 1000  # Convert to ms

                  if base:
                      base_mean = base['stats']['mean'] * 1000
                      change = ((curr_mean - base_mean) / base_mean) * 100
                      change_str = f'+{change:.1f}%' if change > 0 else f'{change:.1f}%'
                      status = 'ðŸ”´' if change > 10 else ('ðŸŸ¡' if change > 5 else 'ðŸŸ¢')
                      print(f'| {name} | {base_mean:.2f} | {curr_mean:.2f} | {status} {change_str} |')
                      if change > 10:
                          regressions.append((name, change))
                  else:
                      print(f'| {name} | N/A | {curr_mean:.2f} | ðŸ†• New |')

              if regressions:
                  print()
                  print('### âš ï¸ Performance Regressions Detected')
                  for name, change in regressions:
                      print(f'- **{name}**: +{change:.1f}% slower')
                  sys.exit(1)
              else:
                  print()
                  print('### âœ… No significant performance regressions')

          except Exception as e:
              print(f'Error comparing benchmarks: {e}')
          "

      - name: Save baseline (main branch only)
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          cp benchmark_results.json .benchmark_baseline.json

      - name: Upload benchmark results
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-results
          path: benchmark_results.json
          retention-days: 30

      - name: Comment on PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let results;
            try {
              results = JSON.parse(fs.readFileSync('benchmark_results.json', 'utf8'));
            } catch (e) {
              console.log('Could not read benchmark results');
              return;
            }

            const benchmarks = results.benchmarks || [];
            if (benchmarks.length === 0) return;

            let body = '## ðŸ“Š Benchmark Results\n\n';
            body += '| Benchmark | Mean (ms) | Min (ms) | Max (ms) | StdDev |\n';
            body += '|-----------|-----------|----------|----------|--------|\n';

            for (const b of benchmarks.slice(0, 20)) {
              const mean = (b.stats.mean * 1000).toFixed(2);
              const min = (b.stats.min * 1000).toFixed(2);
              const max = (b.stats.max * 1000).toFixed(2);
              const stddev = (b.stats.stddev * 1000).toFixed(3);
              body += `| ${b.name} | ${mean} | ${min} | ${max} | ${stddev} |\n`;
            }

            if (benchmarks.length > 20) {
              body += `\n*...and ${benchmarks.length - 20} more benchmarks*\n`;
            }

            body += '\n<details><summary>Performance Targets</summary>\n\n';
            body += '- Constitution parsing: <100ms\n';
            body += '- Rule validation: <1ms per rule\n';
            body += '- Intent parsing: <10ms\n';
            body += '- Agent routing: <100ms\n';
            body += '- Memory operations: <100ms\n';
            body += '- Orchestration overhead: <2 seconds\n';
            body += '</details>';

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

  benchmark-summary:
    name: Generate Summary
    runs-on: ubuntu-latest
    needs: benchmark
    if: always()
    steps:
      - name: Download results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
        continue-on-error: true

      - name: Generate summary
        run: |
          if [ -f benchmark_results.json ]; then
            echo "## ðŸ“Š Benchmark Summary" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            python3 -c "
          import json
          with open('benchmark_results.json') as f:
              data = json.load(f)

          benchmarks = data.get('benchmarks', [])
          print(f'Total benchmarks: {len(benchmarks)}')
          print()
          print('### Top 10 Slowest Benchmarks')
          print()
          print('| Benchmark | Mean (ms) |')
          print('|-----------|-----------|')

          sorted_benchmarks = sorted(benchmarks, key=lambda x: x['stats']['mean'], reverse=True)
          for b in sorted_benchmarks[:10]:
              mean_ms = b['stats']['mean'] * 1000
              print(f'| {b[\"name\"]} | {mean_ms:.2f} |')
            " >> $GITHUB_STEP_SUMMARY
          else
            echo "No benchmark results available" >> $GITHUB_STEP_SUMMARY
          fi
